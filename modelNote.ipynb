{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_boxes(frame, xyxy, label):  # plot detected class box\n",
    "    x1 = int(xyxy[0])\n",
    "    y1 = int(xyxy[1])\n",
    "    x2 = int(xyxy[2])\n",
    "    y2 = int(xyxy[3])\n",
    "\n",
    "    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "    frame = cv2.rectangle(frame, (x1, y1 - 20), (x1 + w, y1), (0,0,255), -1)\n",
    "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    frame = cv2.putText(frame, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def save_handled_frame(frame_id, saved_video_path, img_save_path):  # save image from saved video with detections\n",
    "    cap = cv2.VideoCapture(saved_video_path)\n",
    "    count = 1\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    while count != frame_id:\n",
    "        count += 1\n",
    "        success, frame = cap.read()\n",
    "\n",
    "    cv2.imwrite(f'{img_save_path}/detection_frame_{frame_id}.jpg', frame)\n",
    "    print('image was saved')\n",
    "\n",
    "\n",
    "class AttentionModel:\n",
    "    def __init__(self):\n",
    "        self.models_keys = {\n",
    "            'detection': 'models/customYolo_phones.pt',  # use yolov8x.pt\n",
    "            'pos_estimation': 'models/yolov8x-pose.pt'\n",
    "        }\n",
    "\n",
    "        # models\n",
    "        self.detect_model = None,\n",
    "        self.detect_model_classes = None\n",
    "        self.pos_est_model = None\n",
    "\n",
    "        # result data\n",
    "        self.detected_data = pd.DataFrame(\n",
    "            columns=['Frame_id', 'Time', 'Objects_class', 'Position'],\n",
    "        )\n",
    "\n",
    "        # video parameters\n",
    "        self.video_type = 'avi'  # saved video type\n",
    "        self.fps = 30\n",
    "        self.sec_per_frame = 1 / self.fps\n",
    "\n",
    "\n",
    "    def load_models(self):\n",
    "        self.detect_model = YOLO(self.models_keys['detection'])\n",
    "        self.detect_model_classes = self.detect_model.names\n",
    "\n",
    "        self.pos_est_model = YOLO(self.models_keys['pos_estimation'])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.detect_model.to('cuda')\n",
    "            self.pos_est_model.to('cuda')\n",
    "\n",
    "\n",
    "    def handle_detection(self, results, frame, frame_id):\n",
    "        detected_classes = []\n",
    "        detected_positions = dict()\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "\n",
    "            for box in boxes:\n",
    "                class_name = self.detect_model_classes[int(box.cls)]\n",
    "                detected_classes.append(str(class_name))\n",
    "                confidence = str(round(box.conf[0].item(), 2))\n",
    "                label = f'{class_name}: {confidence}'\n",
    "\n",
    "                xyxy = box.xyxy[0]\n",
    "                frame = plot_boxes(frame, xyxy, label)\n",
    "\n",
    "                if not class_name in detected_positions.keys():\n",
    "                    detected_positions[class_name] = []\n",
    "                detected_positions[class_name].append(xyxy)\n",
    "\n",
    "        detected_classes = set(detected_classes)\n",
    "\n",
    "        if detected_classes:\n",
    "            detection_time = frame_id * self.sec_per_frame\n",
    "            self.detected_data.loc[len(self.detected_data.index)] = [frame_id, detection_time, detected_classes, detected_positions]\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def handle_pos_est(self, results, frame, frame_id):\n",
    "        # plot_pos_points\n",
    "        return\n",
    "\n",
    "\n",
    "    def predict(self, data_path, out_path, detection=True, pos_estimation=False):\n",
    "        cap = cv2.VideoCapture(data_path)\n",
    "\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "\n",
    "        codec = cv2.VideoWriter_fourcc('M','J','P','G')  # avi format\n",
    "        out = cv2.VideoWriter(out_path , codec, self.fps, (frame_width, frame_height))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        frame_count = 0\n",
    "\n",
    "        while success:\n",
    "            frame_count += 1\n",
    "\n",
    "            if detection:\n",
    "                detection_results = self.detect_model(frame)\n",
    "                frame = self.handle_detection(detection_results, frame, frame_count)\n",
    "\n",
    "            if pos_estimation:\n",
    "                pos_est_results = self.pos_est_model(frame)\n",
    "                frame = self.handle_pos_est(pos_est_results, frame, frame_count)\n",
    "\n",
    "            out.write(frame)\n",
    "            success, frame = cap.read()\n",
    "\n",
    "        end = time.time() - start\n",
    "        print(f'Time: {end}')\n",
    "\n",
    "    def get_detected_data(self):\n",
    "        return self.detected_data\n",
    "\n",
    "    def clear_data(self):  # clear detected_data\n",
    "        self.detected_data = self.detected_data.iloc[0:0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUT_PATH = 'result/model_output_video.avi' # your video path for saving\n",
    "DATA_PATH = 'test_data/phone_test_3.mp4' # your video path for processing\n",
    "SAVE_IMG_PATH = 'result'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AttentionModel()\n",
    "model.load_models()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.predict(DATA_PATH, OUT_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pose-estimator\n",
    "heavy_model = YOLO('models/yolov8x-pose.pt')\n",
    "heavy_model.to('cuda')\n",
    "heavy_model.predict('VIDEO_PATH', stream=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
