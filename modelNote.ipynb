{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "keyPoints = {\n",
    "    'NOSE': 0,\n",
    "    'LEFT_EYE': 1,\n",
    "    'RIGHT_EYE': 2,\n",
    "    'LEFT_EAR':3,\n",
    "    'RIGHT_EAR': 4,\n",
    "    'LEFT_SHOULDER': 5,\n",
    "    'RIGHT_SHOULDER':6,\n",
    "    'LEFT_ELBOW': 7,\n",
    "    'RIGHT_ELBOW': 8,\n",
    "    'LEFT_WRIST': 9,\n",
    "    'RIGHT_WRIST': 10,\n",
    "    'LEFT_HIP': 11,\n",
    "    'RIGHT_HIP': 12,\n",
    "    'LEFT_KNEE': 13,\n",
    "    'RIGHT_KNEE': 14,\n",
    "    'LEFT_ANKLE': 15,\n",
    "    'RIGHT_ANKLE': 16,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def plot_boxes(frame, xyxy, label):  # plot detected class box\n",
    "    x1 = int(xyxy[0])\n",
    "    y1 = int(xyxy[1])\n",
    "    x2 = int(xyxy[2])\n",
    "    y2 = int(xyxy[3])\n",
    "\n",
    "    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "    frame = cv2.rectangle(frame, (x1, y1 - 20), (x1 + w, y1), (0,0,255), -1)\n",
    "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    frame = cv2.putText(frame, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "    return frame\n",
    "\n",
    "def save_handled_frame(frame_id, saved_video_path, img_save_path):\n",
    "    cap = cv2.VideoCapture(saved_video_path)\n",
    "    count = 1\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    while count != frame_id:\n",
    "        count += 1\n",
    "        success, frame = cap.read()\n",
    "\n",
    "    cv2.imwrite(f'{img_save_path}/detection_frame_{frame_id}.jpg', frame)\n",
    "    print('image was saved')\n",
    "\n",
    "\n",
    "class AttentionModel:\n",
    "    def __init__(self):\n",
    "        self.models_keys = {\n",
    "            'detection': 'models/yolov8x.pt',\n",
    "            'pos_estimation': 'models/yolov8x-pose.pt'\n",
    "        }\n",
    "\n",
    "        # models\n",
    "        self.detect_model = None,\n",
    "        self.detect_model_classes = None\n",
    "        self.pos_est_model = None\n",
    "\n",
    "        self.detected_data = pd.DataFrame(\n",
    "            columns=['Frame_id', 'Time', 'Objects_class', 'Position'],\n",
    "        )\n",
    "        self.pos_est_data = pd.DataFrame(\n",
    "            columns=['Frame_id', 'Time', 'Keypoints'],\n",
    "        )\n",
    "\n",
    "        # video parameters\n",
    "        self.video_type = 'avi'  # saved video type\n",
    "        self.fps = 30\n",
    "        self.sec_per_frame = 1 / self.fps\n",
    "\n",
    "\n",
    "    def load_models(self):\n",
    "        self.detect_model = YOLO(self.models_keys['detection'])\n",
    "        self.detect_model_classes = self.detect_model.names\n",
    "\n",
    "        self.pos_est_model = YOLO(self.models_keys['pos_estimation'])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.detect_model.to('cuda')\n",
    "            self.pos_est_model.to('cuda')\n",
    "\n",
    "\n",
    "    def handle_detection(self, results, frame, frame_id, save=False):\n",
    "        detected_classes = []\n",
    "        detected_positions = dict()\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "\n",
    "            for box in boxes:\n",
    "                class_name = self.detect_model_classes[int(box.cls)]\n",
    "                detected_classes.append(str(class_name))\n",
    "                xyxy = box.xyxy[0]\n",
    "\n",
    "                if save:\n",
    "                    confidence = str(round(box.conf[0].item(), 2))\n",
    "                    label = f'{class_name}: {confidence}'\n",
    "                    frame = plot_boxes(frame, xyxy, label)\n",
    "\n",
    "                if not class_name in detected_positions.keys():\n",
    "                    detected_positions[class_name] = []\n",
    "                detected_positions[class_name].append(xyxy)\n",
    "\n",
    "        detected_classes = set(detected_classes)\n",
    "\n",
    "        if detected_classes:\n",
    "            detection_time = frame_id * self.sec_per_frame\n",
    "            self.detected_data.loc[len(self.detected_data.index)] = [frame_id, detection_time, detected_classes, detected_positions]\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def handle_pos_est(self, results, frame, frame_id, save=False):\n",
    "        for result in results:\n",
    "            keypoints = result.keypoints.xy.cpu().numpy()[0]\n",
    "\n",
    "            if save:\n",
    "                for point in keypoints:\n",
    "                    frame = cv2.circle(frame, (int(point[0]), int(point[1])), radius=0, color=(0, 255, 0), thickness=10)\n",
    "\n",
    "            detection_time = frame_id * self.sec_per_frame\n",
    "            self.pos_est_data.loc[len(self.detected_data.index)] = [frame_id, detection_time, keypoints]\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def process_video(self, data_path, out_path, detection=True, pos_estimation=False, save=False):\n",
    "        cap = cv2.VideoCapture(data_path)\n",
    "\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "\n",
    "        codec = cv2.VideoWriter_fourcc('M','J','P','G')  # avi format\n",
    "        out = cv2.VideoWriter(out_path , codec, self.fps, (frame_width, frame_height))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        frame_count = 0\n",
    "\n",
    "        while success:\n",
    "            frame_count += 1\n",
    "\n",
    "            if detection:\n",
    "                detection_results = self.detect_model(frame)\n",
    "                frame = self.handle_detection(detection_results, frame, frame_count, save)\n",
    "\n",
    "            if pos_estimation:\n",
    "                pos_est_results = self.pos_est_model(frame)\n",
    "                frame = self.handle_pos_est(pos_est_results, frame, frame_count, save)\n",
    "\n",
    "            if save:\n",
    "                out.write(frame)\n",
    "            success, frame = cap.read()\n",
    "        end = time.time() - start\n",
    "        print(f'Time: {end}')\n",
    "\n",
    "\n",
    "    def get_detected_data(self):\n",
    "        return self.detected_data\n",
    "\n",
    "    def clear_data(self):  # clear detected_data\n",
    "        self.detected_data = self.detected_data.iloc[0:0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "OUT_PATH = 'result/model_output_video.avi' # your video path for saving\n",
    "DATA_PATH = 'test_data/phone_test_3.mp4' # your video path for processing\n",
    "SAVE_IMG_PATH = 'result'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\customYolo_phones.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m AttentionModel()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 51\u001B[0m, in \u001B[0;36mAttentionModel.load_models\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_models\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetect_model \u001B[38;5;241m=\u001B[39m \u001B[43mYOLO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels_keys\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdetection\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetect_model_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetect_model\u001B[38;5;241m.\u001B[39mnames\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_est_model \u001B[38;5;241m=\u001B[39m YOLO(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_keys[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpos_estimation\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\ultralytics\\engine\\model.py:92\u001B[0m, in \u001B[0;36mModel.__init__\u001B[1;34m(self, model, task)\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(model, task)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 92\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\ultralytics\\engine\\model.py:137\u001B[0m, in \u001B[0;36mModel._load\u001B[1;34m(self, weights, task)\u001B[0m\n\u001B[0;32m    135\u001B[0m suffix \u001B[38;5;241m=\u001B[39m Path(weights)\u001B[38;5;241m.\u001B[39msuffix\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m suffix \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pt\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mckpt \u001B[38;5;241m=\u001B[39m \u001B[43mattempt_load_one_weight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moverrides \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset_ckpt_args(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs)\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:588\u001B[0m, in \u001B[0;36mattempt_load_one_weight\u001B[1;34m(weight, device, inplace, fuse)\u001B[0m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattempt_load_one_weight\u001B[39m(weight, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, fuse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 588\u001B[0m     ckpt, weight \u001B[38;5;241m=\u001B[39m \u001B[43mtorch_safe_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# load ckpt\u001B[39;00m\n\u001B[0;32m    589\u001B[0m     args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mDEFAULT_CFG_DICT, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_args\u001B[39m\u001B[38;5;124m'\u001B[39m, {}))}  \u001B[38;5;66;03m# combine model and default args, preferring model args\u001B[39;00m\n\u001B[0;32m    590\u001B[0m     model \u001B[38;5;241m=\u001B[39m (ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mema\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m ckpt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()  \u001B[38;5;66;03m# FP32 model\u001B[39;00m\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:527\u001B[0m, in \u001B[0;36mtorch_safe_load\u001B[1;34m(weight)\u001B[0m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    523\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m temporary_modules({\n\u001B[0;32m    524\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.yolo.utils\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.utils\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    525\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.yolo.v8\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.models.yolo\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    526\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.yolo.data\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124multralytics.data\u001B[39m\u001B[38;5;124m'\u001B[39m}):  \u001B[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001B[39;00m\n\u001B[1;32m--> 527\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m, file  \u001B[38;5;66;03m# load\u001B[39;00m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# e.name is missing module name\u001B[39;00m\n\u001B[0;32m    530\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\torch\\serialization.py:791\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    789\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    793\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    795\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    796\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\torch\\serialization.py:271\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mC:\\temp\\Anaconda\\lib\\site-packages\\torch\\serialization.py:252\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 252\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'models\\\\customYolo_phones.pt'"
     ]
    }
   ],
   "source": [
    "model = AttentionModel()\n",
    "model.load_models()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.process_video(DATA_PATH, OUT_PATH, pos_estimation=True, detection=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.pos_est_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.detected_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pose-estimator\n",
    "heavy_model = YOLO('models/yolov8x-pose.pt')\n",
    "heavy_model.to('cuda')\n",
    "heavy_model.predict('VIDEO_PATH', stream=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pose-estimator\n",
    "heavy_model = YOLO('models/yolov8x-pose.pt')\n",
    "heavy_model.to('cuda')\n",
    "result = heavy_model.predict(DATA_PATH, stream=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
